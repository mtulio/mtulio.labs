#!/usr/bin/env bash

#
# (WIP) Resize OpenShift Machines
# - Follow steps to resize machine described here:
#   https://mtulio.net/playbooks/openshift/resize-machines/
#

set -o pipefail
set -o nounset
set -o errexit

# ######
# Global
# ######

declare -g CACHE_DIR="${HOME}/.cache/oc_plugin-machine-resize"
declare -g CACHE_FILE_MACHINES="${CACHE_DIR}/machines.json"
declare -g CACHE_FILE_NODES="${CACHE_DIR}/nodes.json"
declare -g CACHE_ETCD="${CACHE_DIR}/etcd-endpoint.json"
declare -g CACHE_CO="${CACHE_DIR}/cluster-operators.json"

declare -g HANDLER_MACHINE=false
declare -g HANDLER_SIZE=false
declare -g HANDLER_LSSIZE=false
declare -g HANDLER_LIST=false

declare -g RT_RESP=false
declare -g RT_YES=true
declare -g RT_NO=false

declare -g OPT_MACHINE=""
declare -g OPT_SIZE=""
declare -g OPT_CONTINUE=false
declare -g OPT_FORCE=false

declare -g CLI_OC=$(command -v oc)
declare -g CLI_AWS=$(command -v aws)
declare -g CLI_AZ=$(command -v az)
declare -g CLI_JQ=$(command -v jq)

set_dep_cloud() {
    # Cloud specifics
    declare -g CLOUD_PROVIDER=$(oc get infrastructures \
        -o jsonpath='{.items[*].status.platformStatus.type}' \
        | tr '[:upper:]' '[:lower:]')

    # Globals by Cloud (and avoid if statements on code)
    if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
        declare -g KEY_PATH_MACHINE_TYPE=".spec.providerSpec.value.instanceType"
        declare -g KEY_MACHINE_TYPE="instanceType"
        declare -g KEY_PATH_MACHINE_ID=".status.providerStatus.instanceId"

    elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
        declare -g KEY_PATH_MACHINE_TYPE=".spec.providerSpec.value.vmSize"
        declare -g KEY_PATH_MACHINE_ID=".status.providerStatus.vmId"
    fi
}

check_dep_required() {
    test -d ${CACHE_DIR} || mkdir -p ${CACHE_DIR}
    test -x ${CLI_JQ} || (
        echo "ERROR: unable to find the 'jq' executable on your PATH. [${CLI_JQ}]"; 
        exit 1
    )
    test -x ${CLI_OC} || (
        echo "ERROR: unable to find the 'oc' executable on your PATH. [${CLI_OC}]"; 
        exit 1
    )
    if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
        test -x ${CLI_AWS} || (
            echo "ERROR: unable to find the 'aws' executable on your PATH. [${CLI_AWS}]"; 
            exit 1
        )
    elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
        test -x ${CLI_AZ} || (
            echo "ERROR: unable to find the 'az' executable on your PATH. [${CLI_AZ}]"; 
            exit 1
        )
    else
        echo "ERROR: this plugin does not support your cloud provider [${CLOUD_PROVIDER}]"
        exit 1
    fi
}

# ###
# AWS
# ###

cache_get_machine_id() {
    local machine_name=${1:-${OPT_MACHINE}}
    local resp=$(jq -r ".items[] \
        | select (.metadata.name==\"${machine_name}\")${KEY_PATH_MACHINE_ID}" \
        ${CACHE_FILE_MACHINES})
    echo ${resp}
}

aws_get_instance_state() {
    local machine_name=${1:-${OPT_MACHINE}}
    local instance_id=$(cache_get_machine_id ${machine_name})
    local current_state=$(aws ec2 describe-instance-status \
        --instance-id ${instance_id} \
        | jq -r '.InstanceStatuses[0].InstanceState.Name')

    if [[ ${current_state} == "null" ]]; then
        echo "not_running"
        return
    fi
    echo ${current_state}
    return
}


aws_change_instance_type() {
    local machine_name=${1:-${OPT_MACHINE}}
    local instance_id=$(cache_get_machine_id ${machine_name})
    local sl_time_sec=30
    local sl_time_total=10
    local sl_time_count=1
    # Avoid error running modify right after Instance is stopped
    # API ERR: An error occurred (IncorrectInstanceState) when \
    # calling the ModifyInstanceAttribute operation: The \
    # instance 'i-0fa478d35649cbfd7' is not in the 'stopped' state.
    #echo " >> Waiting [${sleep_time_sec}s] power state be propagated to AWS EC2 API..."
    #sleep ${sleep_time_sec};
    echo " >> Changing instance [${instance_id}] to type [${OPT_SIZE}]..."

    # disable safeguard and implement err check
    set +o errexit
    while true; do
        aws ec2 modify-instance-attribute \
            --instance-id ${instance_id} \
            --instance-type ${OPT_SIZE}
        RC=$?
        if [[ ${RC} -eq 0 ]]; then
            echo " >> SUCCESS: type changed from Cloud Provider"
            break
        fi
        if [[ ${RC} -eq 255 ]]; then
            echo " >> ERROR [${RC}] detected when sending modify-instance-attribute"
            echo " >> Sleeping [${sl_time_sec}s] [${sl_time_count}/${sl_time_total}]..."
            sleep ${sl_time_sec}
        fi
        if [[ "${sl_time_count}" == "${sl_time_total}" ]]; then
            echo " >> ERROR timeout waiting the Instance to be available to be modified."
            exit 1
        fi
        sl_time_count=$(echo "${sl_time_count} + 1" |bc)
    done
    set -o errexit
}

aws_power_on_instance() {
    local machine_name=${1:-${OPT_MACHINE}}
    local instance_id=$(cache_get_machine_id ${machine_name})
    echo " >> Starting EC2..."
    aws ec2 start-instances \
        --instance-ids ${instance_id}
}

# #####
# Azure
# #####

azure_get_instance_state() {
    local machine_name=${1:-${OPT_MACHINE}}
    local instance_id=$(cache_get_machine_id ${machine_name})
    local current_state=$(aws ec2 describe-instance-status \
        --instance-id ${instance_id} \
        | jq -r '.InstanceStatuses[0].InstanceState.Name')

    if [[ ${current_state} == "\"PowerState/stopped\"" ]]; then
        echo "not_running"
        return
    fi
    if [[ ${current_state} == "\"PowerState/running\"" ]]; then
        echo "running"
        return
    fi
    echo ${current_state}
    return
}

azure_change_instance_type() {
    local machine_name=${1:-${OPT_MACHINE}}
    local instance_id=$(cache_get_machine_id ${machine_name})
    local resource_group=""
    az vm resize \
        --resource-group ${resource_group} \
        --name ${machine_name} \
        --size ${OPT_SIZE}
}

# ##
# OC
# ##

cache_update_get_machines() {
    oc get machines \
        -n openshift-machine-api \
        -l machine.openshift.io/cluster-api-machine-role=master \
        -o json > ${CACHE_FILE_MACHINES}
}

cache_update_get_nodes() {
    oc get nodes \
        -l kubernetes.io/os=linux,node-role.kubernetes.io/master= \
        -o json > ${CACHE_FILE_NODES}
}

cache_update_get_co() {
    oc get co -o json > ${CACHE_CO}
}

cache_update_get_etcd() {
    # rsh was failing when a member is failing (returning 1, force does not work)
    set +o pipefail
    set +o errexit
    oc -n openshift-etcd exec \
        $(oc get pods \
            -n openshift-etcd \
            -l app=etcd \
            -o jsonpath='{.items[0].metadata.name}') \
        -- etcdctl endpoint status -w json 2>/dev/null > ${CACHE_ETCD} || true

    set -o pipefail
    set -o errexit
}

cache_get_node_name() {
    local machine_name=${1:-${OPT_MACHINE}}
    jq -r ".items[] \
        | select (.metadata.name==\"${machine_name}\").status.nodeRef.name" \
        ${CACHE_FILE_MACHINES}
}

cache_get_node_status() {
    local machine_name=${1:-${OPT_MACHINE}}
    local node_name=$(cache_get_node_name ${machine_name})
    local resp=$(jq -r ".items[] \
        | select (.metadata.name==\"${node_name}\").status.conditions[] \
        | select (.type==\"Ready\").status" \
        ${CACHE_FILE_NODES})
    if [[ "${resp}" == "Unknown" ]]; then
        echo "NotReady"
        return
    fi
    if [[ "${resp}" == "True" ]]; then
        echo "Ready"
        return
    fi
    echo "${resp}"
}

cache_get_machine_phase() {
    local machine_name=${1:-${OPT_MACHINE}}
    jq -r ".items[] \
        | select (.metadata.name==\"${machine_name}\").status.phase" \
        ${CACHE_FILE_MACHINES}
}

cache_get_machine_node_ip() {
    local machine_name=${1:-${OPT_MACHINE}}
    jq -r ".items[] \
        | select (.metadata.name==\"${machine_name}\").status.addresses[] \
        | select (.type==\"InternalIP\").address " \
        ${CACHE_FILE_MACHINES}
}

cache_get_machine_size_cloud() {
    local machine_name=${1:-${OPT_MACHINE}}
    jq -r ".items[] \
        | select (\
            .metadata.name==\"${machine_name}\"\
        ).metadata.labels.\"machine.openshift.io/instance-type\"" \
        ${CACHE_FILE_MACHINES}
}

cache_get_etcd_field_isleader() {
    jq -r '.[] | ( .Status.header.member_id == .Status.leader  )' ${CACHE_ETCD}
}

# 1=list 2=string
list_contains_str() {
    [[ ${1} =~ (^|[[:space:]])${2}($|[[:space:]]) ]]\
        && RT_RESP=${RT_YES} \
        || RT_RESP=${RT_NO}
}

run_oc_cordon_node() {
    local machine_name=${OPT_MACHINE}
    local node_name=$(cache_get_node_name ${machine_name})
    
    echo -e " > Cordoning the node [${node_name}]..."
    if [[ "$(cache_get_node_status)" != "Ready" ]]; then
        echo -ne " >> Node is not Ready, ignoring cordon..."
        RT_RESP=${RT_YES}
        return
    fi
    oc adm cordon ${node_name}

    echo -ne " > Node Cordoned [${node_name}]..."
    RT_RESP=${RT_YES}
}

run_oc_uncordon_node() {
    local machine_name=${OPT_MACHINE}
    local node_name=$(cache_get_node_name ${machine_name})
    
    echo -e " > UnCordoning the node [${node_name}]..."
    if [[ "$(cache_get_node_status)" != "Ready" ]]; then
        echo -ne " >> Node is NotReady, ignoring cordon..."
        RT_RESP=${RT_YES}
        return
    fi
    oc adm uncordon ${node_name}

    echo -ne " >> Node Uncordoned [${node_name}]..."
    RT_RESP=${RT_YES}
}

run_oc_drain_node() {
    local machine_name=${OPT_MACHINE}
    local node_name=$(cache_get_node_name ${machine_name})
    
    echo -e " > Draining the node [${node_name}]..."
    if [[ "$(cache_get_node_status)" != "Ready" ]]; then
        echo -ne " >> Node is not Ready, ignoring drain..."
        RT_RESP=${RT_YES}
        return
    fi
    oc adm drain ${node_name} \
        --ignore-daemonsets \
        --grace-period=60 \
        --delete-local-data || (
            RT_RESP=${RT_NO}
            return
        )

    echo -ne " > Node drained [${node_name}]..."
    RT_RESP=${RT_YES}
    return
}

run_machine_power_off() {
    local machine_name=${OPT_MACHINE}
    local node_name=$(cache_get_node_name ${machine_name})

    echo -e " > Powering off the node [${node_name}]..."
    if [[ "$(cache_get_node_status)" != "Ready" ]]; then
        echo " >> Node is not Ready, ignoring shutdown..."
        RT_RESP=${RT_YES}
        return
    fi

    oc debug node/${node_name} -- chroot /host shutdown -h 1 || true

    echo -ne " > Shutdown command was sent to node [${node_name}]..."
    RT_RESP=${RT_YES}
    return
}

run_wait_node_notready() {
    local machine_name=${OPT_MACHINE}
    local node_name=$(cache_get_node_name ${machine_name})
    echo -e " > Waiting the node be NotReady..."

    while true; do
        cache_update_get_nodes
        local node_status=$(cache_get_node_status)
        if [[ ${node_status} != "Ready" ]]; then
            echo " >> Node is NotReady [${node_status}]...continuing the process...";
            RT_RESP=${RT_YES}
            return
        fi
        echo " >> Node is Ready [${node_status}]...waiting 15s";
        sleep 15
    done
    RT_RESP=${RT_YES}
}

run_wait_node_ready() {
    local machine_name=${OPT_MACHINE}
    local node_name=$(cache_get_node_name ${machine_name})
    echo -e " > Waiting the node be Ready..."

    while true; do
        cache_update_get_nodes
        local node_status=$(cache_get_node_status)
        if [[ ${node_status} == "Ready" ]]; then
            echo " >> Node is Ready [${node_status}]...continuing the process...";
            RT_RESP=${RT_YES}
            return
        fi
        echo " >> Node is NotReady [${node_status}]...waiting 15s";
        sleep 15
    done
    RT_RESP=${RT_YES}
}

run_wait_node_stopped() {
    echo -e " > Waiting the Instance to report stopped on Cloud Provider [${CLOUD_PROVIDER}]..."
    while true; do \
        if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
            st=$(aws_get_instance_state)
        elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
            st=$(azure_get_instance_state)
        fi
        test $st == "not_running" && break;
        echo " >> Instance still running [$st]; sleeping 15s"
        sleep 15;
    done
    echo " >> Node reported as not running [$st], following to the next step..."
    RT_RESP=${RT_YES}
}

run_wait_node_started() {
    echo -e " > Waiting the Instance to report started on Cloud Provider [${CLOUD_PROVIDER}]..."
    while true; do \
        if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
            st=$(aws_get_instance_state)
        elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
            st=$(azure_get_instance_state)
        fi
        test $st == "running" && break;
        echo " >> Instance still not running [$st]; sleeping 15s"
        sleep 15;
    done
    echo " >> Node reported as running [$st], following to the next step..."
    RT_RESP=${RT_YES}
}

run_machine_power_on() {
    echo " > Sending power on command to Cloud Provider [${CLOUD_PROVIDER}]..."
    if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
        aws_power_on_instance
    elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
        aws_power_on_vm
    fi
    RT_RESP=${RT_YES}
}

run_change_instance_type() {
    echo " > Running Instance resize on Cloud Provider [${CLOUD_PROVIDER}]..."
    if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
        aws_change_instance_type
    elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
        azure_change_instance_type
    fi
    RT_RESP=${RT_YES}
}

run_wait_machine_reconciliation() {
    local machine_name=${OPT_MACHINE}
    while true; do 
        cache_update_get_machines
        local machine_size=$(cache_get_machine_size_cloud ${machine_name})
        test "${machine_size}" == "${OPT_SIZE}" && break;
        echo " >> Instance size was not updated [${machine_size}]; sleeping 15s"
        sleep 15;
    done
    echo " > Instance size reports as [${machine_size}], following to the next step..."
    RT_RESP=${RT_YES}
}

run_patch_machine_size() {
    local machine_name=${OPT_MACHINE}
    local machine_size_cloud=$(cache_get_machine_size_cloud ${machine_name})

    echo -e " > Starting the Machine Size patch..."
    echo -e " >> Check target: target=[${OPT_SIZE}] machine_cloud=[${machine_size_cloud}]"
    if [[ "${OPT_SIZE}" != "${machine_size_cloud}" ]]; then
        echo " >> Target Machine size missmatch with Cloud Provider discovered on reconciliation...please check it."
        exit 1
    fi

    oc patch machine ${machine_name} \
        -n openshift-machine-api \
        --type=merge \
        -p "{\"spec\":{\"providerSpec\":{\"value\":{\"${KEY_MACHINE_TYPE}\":\"${OPT_SIZE}\"}}}}"

    echo -e " >> Machine patched with new size [${OPT_SIZE}]..."
    RT_RESP=${RT_YES}
    return
}

# ######
# COMMON
# ######

# Check if a given IP is a etcd leader, return a boolean with true for leader.
check_etcd_leader_by_ip() {
    local check_ip="$1"
    local resp=$(
        jq -r ".[] | \
            {
                endpoint_ip: ( .Endpoint | capture(\"https://(?<ip>[[:digit:]].+):2379\").ip ),\
                member_id: ( .Status.header.member_id),\
                is_leader: ( .Status.header.member_id == .Status.leader)\
            }" ${CACHE_ETCD} \
            | jq -r ". | select(.endpoint_ip==\"${check_ip}\").is_leader")
    if [[ -z "${resp}" ]]; then
        echo "false"
        return
    fi
    echo "${resp}"
    return
}

check_machine_was_not_resized_cloud() {
    echo -ne " > Checking ig machine was already resized on Cloud provider..."
    local machine_type_cloud=$(jq -r \
        ".items[] \
         | select (.metadata.name==\"${OPT_MACHINE}\").metadata.labels.\"machine.openshift.io/instance-type\"" \
         ${CACHE_FILE_MACHINES})

    if [[ "${machine_type_cloud}" != "${OPT_SIZE}" ]]; then
        RT_RESP=${RT_YES}
        return
    fi

    # check if the process was started
    local machine_type_spec=$(\
        jq -r ".items[] | \
            select (\
                .metadata.name==\"${machine_name}\"\
            )${KEY_PATH_MACHINE_TYPE}" \
        ${CACHE_FILE_MACHINES})

    if [[ "${machine_type_spec}" == "${OPT_SIZE}" ]]; then
        echo "Machine was already resized."
        echo " '> target_size=[${OPT_SIZE}] current_size=[${machine_type_spec}]"
        exit 0
    fi

    echo "ToDo: flow to jump to machine_patch"

    RT_RESP=${RT_YES}
}

check_node_is_ready() {
    echo -ne " > Checking if Node is Ready..."
    local node_status=$(cache_get_node_status)
    if [[ ${node_status} == "Ready" ]]; then
        RT_RESP=${RT_YES}
        return
    fi
    RT_RESP=${RT_NO}
}

check_machine_is_running() {
    echo -ne " > Checking if Machine is Running..."
    local machine_phase=$(cache_get_machine_phase)
    if [[ ${machine_phase} == "Running" ]]; then
        RT_RESP=${RT_YES}
        return
    fi
    RT_RESP=${RT_NO}
}

check_etcd_has_leader() {
    echo -ne " > Checking if etcd has a leader..."
    local node_leaders=($(cache_get_etcd_field_isleader))

    has_leader=false
    for leader_st in  ${node_leaders[@]}; do
        if [[ ${leader_st} == true ]]; then
            has_leader=true
            break
        fi
    done
    if [[ ${has_leader} == true ]]; then
        RT_RESP=${RT_YES}
        return
    fi
    RT_RESP=${RT_NO}
}

check_etcd_has_only_one_leader() {
    echo -ne " > Checking if etcd has only one leader..."
    local node_leaders=($(cache_get_etcd_field_isleader))
    local cnt_leaders=$(echo ${etcd_leaders[@]} |tr ' ' '\n' |uniq -c |grep true |awk '{print$1}')
    if [[ ${cnt_leaders} -eq 1 ]] ; then
        RT_RESP=${RT_YES}
        return
    fi
    RT_RESP=${RT_NO}
}

check_etcd_machine_is_not_leader() {
    echo -ne " > Checking if Machine is not a etcd leader..."
    local machine_ip=$(cache_get_machine_node_ip ${OPT_MACHINE})
    local is_etcd_leader=$(check_etcd_leader_by_ip ${machine_ip})
    if [[ ${is_etcd_leader} == false ]]; then
        RT_RESP=${RT_YES}
        return
    fi
    RT_RESP=${RT_NO}
    return
}

#> Check resps
check_resp() {
    local resp=$1

    # Test(s) success or failed
    if [[ "${resp}" == "${RT_YES}" ]]; then
        echo "SUCCESS"
        return
    fi
    echo "FAIL";
    exit 1;
}

check_resp_cont() {
    local resp=$1

    # Tests failed but --continue=true
    if [[ "${OPT_CONTINUE:-}" == false ]] && [[ "${resp}" == "${RT_NO}" ]]; then
        echo "FAIL. --continue flag was not set."
        exit 1
    fi
    if [[ "${OPT_CONTINUE:-}" == true ]] && [[ "${resp}" == "${RT_YES}" ]]; then
        echo "FAIL. --continue flag was found. Skiping FAIL state."
        return
    fi
    #echo ">> ${resp}"
    check_resp ${RT_YES}
}

check_resp_force() {
    local resp=$1

    # Tests failed but --force=true
    if [[ "${OPT_FORCE:-}" == false ]] && [[ ${resp} == ${RT_NO} ]]; then
        echo "FAIL. --force flag was not set."
        exit 1
    fi
    if [[ "${OPT_FORCE:-}" == true ]] && [[ "${resp}" == "${RT_NO}" ]]; then
        echo "FAIL. --force flag was set. Skiping FAIL state."
        return
    fi
    check_resp ${resp}
}

# ###
# CLI
# ###

resize_machine_runner() {

    echo "Starting resize process to machine [${OPT_MACHINE}] as target size [${OPT_SIZE}]"

    # Check if Machine was already resized (TYPE_CLOUD==--size)
    # '-> Check if Machine was already resized (TYPE_SPEC==--size) if not -> patch Machine
    check_machine_was_not_resized_cloud; check_resp ${RT_RESP}

    # Check Node is Ready (--continue)
    check_node_is_ready; check_resp_cont ${RT_RESP}

    # Check Machine is Running (--continue)
    check_machine_is_running; check_resp_cont ${RT_RESP}

    # Check if etcd cluster has a leader (ca we use forced flag?)
    check_etcd_has_leader; check_resp_force ${RT_RESP}

    # Check if etcd cluster has only one leader (?)
    
    # Check if machine it's not a etcd_leader (otherwise --force)
    check_etcd_machine_is_not_leader; check_resp_force ${RT_RESP}

    # Validation complete
    echo " > ALl validation was complete, starting the resize..."

    # ask to continue
    # ToDo

    # cordon node
    run_oc_cordon_node; check_resp_force ${RT_RESP}

    # drain node
    run_oc_drain_node; check_resp_force ${RT_RESP}

    # shutdown node
    run_machine_power_off; check_resp ${RT_RESP}

    # wait until node is NotReady
    run_wait_node_notready;

    # wait until instance/VM is stopped
    run_wait_node_stopped;

    # change instance type
    run_change_instance_type;

    # start machine
    run_machine_power_on;

    # wait VM is running
    run_wait_node_started;

    # wait Node is Ready
    run_wait_node_ready;

    # uncordon
    run_oc_uncordon_node; check_resp ${RT_RESP}

    # wait Machine is Reconciled (TYPE_CLOUD==--size)
    run_wait_machine_reconciliation

    # patch Machine API object
    run_patch_machine_size

    # wait CO is not degraded
    # '-> check if pods is running (kube-api + etcd) : do we need? CO look to it
    # check if all etcd member is started
    # check if all etcd member is healthy
    # check if etcd cluster has a leader
    # Check if etcd cluster has only one leader
}

# Collect all information and resize the machine
cmd_resize_machine() {
    echo "Collecting current cluster state/objects to resize flow..."
    cache_update_get_machines
    cache_update_get_nodes
    cache_update_get_etcd
    cache_update_get_co

    resize_machine_runner
}

cmd_list_sizes() {
    echo "ToDo: List Sizes from Cloud Provider."
}

# Collect and show all machine+node information to resize
cmd_list_machines() {

    cache_update_get_machines
    cache_update_get_nodes
    cache_update_get_etcd

    echo "Cloud Provider: ${CLOUD_PROVIDER}"
    printf "%15s \t%15s \t%15s \t\t%10s \t%10s \t%10s \t%10s \t%10s \t%10s\n" \
        "MACHINE_NAME" "MACHINE_ID" "NODE_NAME"  \
        "PHASE" "NODE_STATUS" \
        "TYPE_SPEC" "TYPE_CLOUD" "RESOURCE_GROUP" \
        "ETCD_LEADER"

    for machine_name in $(jq -r '.items[].metadata.name' ${CACHE_FILE_MACHINES}); do
        node_name=$(cache_get_node_name ${machine_name})
        node_status=$(cache_get_node_status ${machine_name})
        #node_status="NotReady"
        #test ${node_status_check} == "True" && node_status="Ready"

        machine_ip=$(cache_get_machine_node_ip ${machine_name})
        machine_id=$(cache_get_machine_id ${machine_name})
        machine_phase=$(cache_get_machine_phase ${machine_name})
        machine_type_spec=$(jq -r ".items[] |select (.metadata.name==\"${machine_name}\")${KEY_PATH_MACHINE_TYPE}" ${CACHE_FILE_MACHINES})
        machine_type_cloud=$(cache_get_machine_size_cloud ${machine_name})
        resource_group="N/A"
        etcd_leader=$(check_etcd_leader_by_ip ${machine_ip})
        
        #if [[ "${CLOUD_PROVIDER}" == "aws" ]]; then
        #    machine_id=$(jq -r ".items[] |select (.metadata.name==\"${machine_name}\") | .status.providerStatus.instanceId" ${CACHE_FILE_MACHINES})
        #    machine_type_spec=$(jq -r ".items[] |select (.metadata.name==\"${machine_name}\") | .spec.providerSpec.value.instanceType" ${CACHE_FILE_MACHINES})
        #elif [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
        if [[ "${CLOUD_PROVIDER}" == "azure" ]]; then
            machine_id=${machine_name}
            #machine_type_spec=$(jq -r ".items[] |select (.metadata.name==\"${machine_name}\") | .spec.providerSpec.value.vmSize" ${CACHE_FILE_MACHINES})
            resource_group=$(jq -r ".items[] |select (.metadata.name==\"${machine_name}\") | .spec.providerSpec.value.resourceGroup" ${CACHE_FILE_MACHINES})
        fi

        printf "%15s \t%15s \t%15s \t%10s  \t%10s \t%10s \t%10s \t%10s \t%10s\n" \
            ${machine_name} ${machine_id} ${node_name} \
            ${machine_phase} ${node_status} \
            ${machine_type_spec} ${machine_type_cloud} ${resource_group} \
            ${etcd_leader}
    done
    exit 0
}

# ####
# MAIN
# ####
function show_help() {

    cat <<-EOF
Usage: ${0} [options]

Available options:
    -N | --machine-name {machine_name} Set Machine name to resize (should match with openshift-machine-api objects).
    -s | --size {new_machine_size}     Set target machine size.
    -l | --list-machines               List available Machines and sizes.
    -L | --list-sizes                  List available sizes for target Machine.
    -h | --help                        Show this help.

Examples:
    # Install this script
    curl -so /usr/local/bin/oc-machine_resize \
        && chmod u+x /usr/local/bin/oc-machine_resize

    # list available Machines
    ${0} --list-machines

    # list available sizes for a given Machine
    ${0} -N machine_name --list-sizes

    # change machine size in AWS
    ${0} --machine-name my-master-0 --size m5.xlarge

    # change machine size in Azure
    ${0} --machine-name my-master-0 --size Standard_D8s_v3

EOF

}

function main() {
    local cmd_target=""

    if [[ ( ${HANDLER_MACHINE} == false ) && \
          ( ${HANDLER_SIZE} == true ) ]]; then
        echo "ERROR: --size {size} should be used with --machine-name {name}"
        exit 1
    fi
    if [[ ( ${HANDLER_MACHINE} == true ) && \
          ( ${HANDLER_SIZE} == false ) && \
          ( ${HANDLER_LSSIZE} == false )]]; then
        echo "ERROR: --size {size} is missing when --machine-name is set"
        exit 1
    fi

    if [[ ( ${HANDLER_MACHINE} == true ) && \
          ( ${HANDLER_SIZE} == true ) ]]; then
        cmd_target="cmd_resize_machine"
    elif [[ ${HANDLER_LSSIZE} == true ]]; then
        cmd_target="cmd_list_sizes"
    elif [[ ${HANDLER_LIST} == true ]]; then
        cmd_target="cmd_list_machines"
    fi

    if [[ -z "${cmd_target}" ]]; then
        echo "ERROR no target option was found."
    fi

    ${cmd_target}
}

function main_cli() {

    # NOTE: This requires GNU getopt.
    # '-> On Mac OS X and FreeBSD need to be installed as: brew install gnu-getopt
    GETOPT_SET=`getopt -n 'oc machine-resize' \
            -o hN:s:lLifc \
            --long help,machine-name:,size:,list,list-sizes,install,force,continue \
            -- "$@"`

    if [ $? != 0 ] ; then
        echo "gnu-getopt seems not to be present. Please install it. Terminating..." >&2 ;
        exit 1 ;
    fi
    eval set -- "${GETOPT_SET}"

    # run pre-req
    

    while true; do
        case "$1" in
            -h | --help         ) show_help; exit 2 ;;
            -N | --machine-name )
                HANDLER_MACHINE=true ; OPT_MACHINE="$2" ; shift 2 ;;
            -s | --size         )
                HANDLER_SIZE=true; OPT_SIZE="$2" ; shift 2 ;;
            -l | --list-machines) 
                HANDLER_LIST=true; shift ;;
            -L | --list-sizes   )
                HANDLER_LSSIZE=true ; shift ;;
            -f | --force        ) OPT_FORCE=true ; shift ;;
            -c | --continue     ) OPT_CONTINUE=true ; shift ;;
            -v | --version      ) OPT_VERSION=true ; shift ;;
            -- ) shift; break ;;
            * ) echo "Option not found: $1"; break ;;
        esac
    done

    set_dep_cloud
    check_dep_required;

    main
}

main_cli "$@"
